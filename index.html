<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<center>
		<br>
		<span style="font-size:36px "><b>mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</b></span>
		<br>
		<br>

		<table align=center width=600px>
			<table align=center width=800px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://ethahtz.github.io/">Tianze Hua</a>*</span>
							<br>
							<span style="font-size:16px">Brown University</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://tttyuntian.github.io/">Tian Yun</a>*</span>
							<br>
							<span style="font-size:16px">Brown University</span>
						</center>
					</td>
			
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=sFyrSa8AAAAJ&hl=en">Ellie Pavlick</a></span>
							<br>
							<span style="font-size:16px">Brown University</span>
						</center>
					</td>
				</tr>
			</table>

			<br>

			<table align="center" width="300px">
				<tbody><tr>
					<td align="center" width="120px">
						<center>
							<div class="buttons" style="margin-bottom: 8px;"><a class="btn btn-primary" role="button" href="TODO_ARXIV_LINK">[Paper]</a></div>
						</center>
					</td>
					<td align="center" width="120px">
						<center>
							<div class="buttons" style="margin-bottom: 8px;"><a class="btn btn-primary" role="button" href="https://github.com/ethahtz/multilingual_othello">[Code]</a></div>
						</center>
					</td>
				</tr>
			</tbody></table>
			
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. 
				<br><br>
				We find that: 
				<ol>
					<li>models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages;</li>
					<li>the introduction of "anchor tokens" (i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and</li>
					<li>the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer.</li>
				</ol>
				Based on our findings, we propose a novel approach -- multilingual pretraining with unified output space -- that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.

			</td>
		</tr>
	</table>

	<br><hr>

	<!-- <hr>
	<center><h1>Video Presentation</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="TODO_VIDEO_LINK" frameborder="0" allowfullscreen align="center"></iframe>
	</p>
	<hr> -->

	<center><h1>Multilingual Othello</h1></center>

	<table align=center width=400px>
		<center><tr>
			<td align=center width=700px>
				<center>
					<img class="round" style="width:430px" src="./resources/mappingFuncs.png" alt="An illustration of the mechanism of mapping functions that map Othello game moves into tokens of specific languages."/>
				</center>
			</td>
			<td align=center width=650px>
				<center>
					<img class="round" style="width:270px" src="./resources/mothello_corpus.png"/>
				</center>
			</td>
		</tr></center>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<br>We use Multilingual Othello (mOthello), a sequence modeling task based on the Othello board game (which has been previously explored in <a href="https://arxiv.org/abs/2210.13382">Othello World</a>) to investigate the essential factors for learning language-neutral representations and whether they are sufficient to facilitate the cross-lingual transfer ability of multilingual models. 
					<br><br>
					In mOthello, a model is given a sequence of game moves in a specific "language", and the task is to predict the next legal move in the same "language". This environment is appropriate for our purposes, since it separates the ground truth "world" (i.e., the game state) which is assumed to be singular, from the language used to describe it, which can take any number of forms (languages). We later formulate our measure of language-neutral representations around the ground-truth underlying world states. The figures above show how mOthello instances can be used to generate a corpus of multilingual data for training and evaluation. 
					<br><br>
					With training data generated via mOthello, we train multilingual models with GPT-2 architecture (mOthelloGPTs) and evaluate whether they have learned language-neutral represetations and whether their performance can transfer across languages.
				</td>
			</tr>
		</center>
	</table>

	<br><hr>

	<center><h1>Measuring Representation Alignment and Cross-lingual Transfer</h1></center>

	<table align=center width=400px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/transfer_probe.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<br>To measure to what extent the hidden representations of semantically similar tokens across languages align with one another, we propose <b>cross-lingual alignment probes</b>, which is a probe <em>P<sub>src</sub></em> trained to recover the board states with input sequences in language <em>L<sub>src</sub></em> to recover the board states given input sequences in another language <em>L<sub>tgt</sub></em>, in a zero-shot fashion. If a cross-lingual alignment probe can reconstruct the board states in another language accurately, this reflects that there is a shared latent space for language <em>L<sub>src</sub></em> and <em>L<sub>tgt</sub></em>.
				</td>
			</tr>
		</center>
	</table>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<br>Cross-lingual transfer ability is the ability to enhance task performance in a target language when being finetuned exclusively with labeled data from the same task, but in a different source language. To measure the cross-lingual transfer ability of mOthelloGPT models, we first pretrain mOthelloGPTs on a prefix-filtered subset of the mOthello corpus, translated to <em>M</em> languages; then, we finetune the pretrained model with a non-prefix-filtered subset of the entire mOthello corpus, but only in one of the languages. We record checkpoints during finetuning process and measure the alignment and performance for each model checkpoint. The performance is measured by calculating the top-1 accuracy of legal move prediction in each language. If a model can achieve better performance in a target language when finetuned solely on the source language, this reflects that the model has good cross-lingual transfer ability.
					<!-- [fn]By prefix-filtered subset, we mean that all the sequences in that subset share the same first few moves. We use prefix-filtered subset as the pretraining corpus because we do not want the pretrained model to generalize too well, hence leaving room for improvement during the finetuning process.[/fn] -->
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Results</h1></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We train a sequence model which simulates the regular setup and blindfolded setup of sequence modeling in BabyAI, named as <em>Complete-State Model</em> and <em>Missing-State Model</em> respectively. 
					<br>
					<em>Randomly initialized baseline</em> is a probe trained on a sequence model with random weights.
					<br>
					<em>Initial state baseline</em> to always predict the initial state, without considering the actions taken by the agent. 
				</td>
			</tr>
		</center>
	</table>
	<br>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<b>1. Role of Intermediate State Observations in the Emergence of State Representations</b>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=1000px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/gotolocal_roles_of_states.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We perform probing on the internal activations of a pretrained Complete-State model and a pretrained Missing-State model.
					For Complete-State model, while the current state is explicitly fed to the model as part of its input, the poor performance of the randomly initialized weights confirms that this information is not trivially preserved in its internal activations - the model needs to be properly pretrained for that to happen. For Missing-State model, even though the current states are not explicitly given, they can be reconstructed reasonably well from the model's internal representations.
				</td>
			</tr>
		</center>
	</table>
	
	<br>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<b>2. Role of Language Instructions in the Emergence of State Representations</b>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=1000px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/gotolocal_roles_of_instructions.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					To study how language instructions impact the emergence of internal representations of states, we pretrain a Complete-State model and a Missing-State model without passing in language instructions. 
					When the language instruction is absent during pretraining, the models struggle to recover state information as accurately as the models trained with language instructions, which reflects that language instructions play an important role in the emergence of internal state representations.
				</td>
			</tr>
		</center>
	</table>
	
	<br>
	
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<b>3. Blindfolded Navigation Performance</b>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=1000px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/gotolocal_navigation_performance.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We further explore the practical implications of the emergence of internal state representations by evaluating pretrained Complete- and Missing-State models on GoToLocal. 
					We observe that Missing-State model performs competitively against Complete-State model, even though Missing-State model only has access to the non-initial/last states during its training. This aligns with our expectation that a model which learns internal representations of states can utilize such representations to predict next actions. 
				</td>
			</tr>
		</center>
	</table>
	
	<br><hr>

	<table align=center width=650px>
		<center><h1>Paper and BibTex</h1></center>
		<tr>
			<td><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></td>
			<td><span style="font-size:14pt">Tianze Hua*, Tian Yun*, Ellie Pavlick.<br>
				<b>mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</b><br>
				In NAACL 2024 (Findings).<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br><br>

	<table align=center width=850px>
		<center>
			<tr>
				<td style="background-color:#e1e1e1">
					<pre><code>
  @inproceedings{hua2024mothello,
	title={mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?},
	author={Tianze Hua and Tian Yun and Ellie Pavlick},
	booktitle={2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	year={2024}
  }
					</code></pre>
				</td>
			</tr>
		</center>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					We would like to thank the anonymous reviewers for their detailed and constructive comments. Our implementation of GPT2 is based on minGPT, and the overall codebase is mostly built on Othello World. Many thanks to Andrej Karpathy and Kenneth Li for open-sourcing their projects! 

					<br><br>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

